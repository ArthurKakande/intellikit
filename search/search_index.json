{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to intellikit","text":"<p>A python toolkit for case based reasoning, information retrieval, natural language processing and other techniques for AI and intelligent systems.</p> <p>\u201cintellikit\u201d is a customisable framework for Case Based Reasoning (CBR) and Information Retrieval (IR) in python. This package is being built primarily for educational purposes, and some content in it may be done more efficiently using Scikit-Learn and other libraries. In some instances such library functions are added directly in \u201cintellikit\u201d but feel free to test out those libraries concurrently and choose what suits your needs best. Some rare similarity measures are implemented from scratch in intellikit but you can extend the functions or implement your own functions depending on your needs. </p> <p>In case you need help getting started, the website for this library can be accessed here! Multiple demo projects are added to the examples tab.</p> <p>If you are new to Case Based Reasoning and Information Retrieval entirely, here a simple refresher for you:</p> <p>Case-Based Reasoning (CBR) is a methodology for solving problems. These  problems may be of a variety of natures. In principle, no problem type is excluded from  being solved with the CBR methodology. The problem types range from exact sciences  to mundane tasks. However, this does not mean that CBR is recommended for all problems.</p> <p>Experiences are essential for CBR. In general, an experience is a recorded episode that occurred in the past, such as \u201cRemember, last time a patient came in with similar symptoms, they had a particular infection\u201d and such experiences are used to help solve future problems or make future decisions. Cases are experiences, they have a context and they also include problems and solutions. A case is explicitly represented/organized using case representations. These can be for example; </p> <ul> <li>Feature-value pairs. A feature value pair is used to represent a state of an entity, for example, colour of an entity, \u201cJessica\u2019s car is red\u201d, where the feature is the colour of the car and the value is red, and the entity is Jessica\u2019s car.</li> <li>Textual case representation (for this representation we consider elements of information retrieval)</li> <li>Object-oriented case representations</li> <li>Graph-based case representations</li> </ul> <p>A key important aspect of CBR is similarity and retrieval. The purpose of retrieval is to retrieve the case from the case base (i.e., a candidate case) that is so similar to a given new problem that their solutions can be swapped. </p> <ul> <li>Free software: MIT License</li> <li>Documentation: https://ArthurKakande.github.io/intellikit</li> </ul>"},{"location":"#feature_value-pairs","title":"Feature_Value Pairs","text":"<p>Simplest representation using key-value pairs to describe a case.</p> <p>Example: Diagnosing a car problem.   Case: \"Car won't start\"   Feature 1: Engine cranks (Yes/No) - Value: No   Feature 2: Warning lights (List) - Value: Battery light   New problem: \"Engine makes a rattling noise\" - Features and values are compared to diagnose the new issue.</p> <p>Use Cases: Simple diagnostic systems, configuration tasks, filtering products based on features.</p>"},{"location":"#textual-case-representation","title":"Textual case representation","text":"<p>Uses natural language to describe a case, similar to a story.</p> <p>Example: Customer service incident reports.   Case: \"Customer reported their computer wouldn't connect to Wi-Fi. Tried restarting the router and device with no success.\"   New problem: Another customer has a similar issue. The textual description helps identify the common cause and solution.</p> <p>Use Cases: Customer service, incident reports, where narratives provide context for problem-solving.</p>"},{"location":"#object-oriented-case-representation","title":"Object-oriented case representation","text":"<p>Cases are represented as objects with attributes, methods, and relationships.</p> <p>Example: Financial loan applications.   Case: Loan application (Object)   Attributes: Applicant name, income, loan amount, credit score.   Methods: Calculate loan eligibility, determine interest rate.   New problem: Another loan application is received. The object structure allows for easy comparison and reuse of past loan decisions based on applicant characteristics.</p> <p>Use Cases: Scenarios with well-defined objects and their interactions, financial analysis, design problems.</p>"},{"location":"#graph-based-case-representation","title":"Graph-based case representation","text":"<p>Cases are represented as nodes (entities) and edges (relationships) in a graph structure.</p> <p>Example: Medical diagnosis.   Case: Patient with a fever (Node) connected to symptoms (Nodes) like cough, sore throat. Edges represent relationships between symptoms.   New problem: Another patient with similar symptoms. The graph allows for reasoning based on connections and identifying potential diagnoses by comparing symptom relationships.</p> <p>Use Cases: Medical diagnosis, network analysis, problems involving complex relationships between entities.</p>"},{"location":"#textstring-attributes-similarity","title":"Text/String Attributes Similarity","text":"<ul> <li>Hamming distance</li> <li>Hamming Similarity (Normalized hamming distance)</li> <li>Levenshtien distance</li> <li>Levenshtien similarity (Normalized levenshtien distance)</li> <li>Level similarity</li> <li>N-grams</li> <li>Cosine Similarity</li> <li>Exact Match (Strings - Not case sensitive)</li> </ul>"},{"location":"#document-retrieval-information-retrieval","title":"Document Retrieval (Information Retrieval)","text":"<ul> <li>Vector space model (TF - IDF) </li> <li>Okapi BM25</li> <li>Sentence Transformers</li> </ul>"},{"location":"#numeric-attribute-similarity","title":"Numeric Attribute Similarity","text":"<ul> <li>City block metric</li> <li>Euclidean distance</li> <li>Exact Match (Symmetric)</li> <li>Log similarity</li> <li>Asymmetric strategy with Case Higher </li> <li>Asymmetric strategy with Query Higher</li> <li>Weighted euclidean distance</li> </ul>"},{"location":"#retrieval","title":"Retrieval","text":"<ul> <li>Linear Retriever</li> <li>Parallel Linear Retriever</li> <li>MACFAC Retriever</li> </ul>"},{"location":"#upcoming-features-that-will-be-added-in-upcoming-releases","title":"Upcoming Features that will be added in upcoming releases","text":"<ul> <li>A question answering module.</li> <li>CBR similarity measures for taxonomies.</li> <li>CBR measures and examples for Object oriented case representations and Graph based representations and Ontologies.</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"common/","title":"common module","text":"<p>The common module contains common functions and classes used by the other modules.</p>"},{"location":"common/#intellikit.common.dataframe_to_dict","title":"<code>dataframe_to_dict(df, orientation='columns')</code>","text":"<p>Convert a DataFrame to a dictionary in various orientations.</p> <ul> <li>df: The DataFrame to convert.</li> <li>orientation: The dictionary orientation.   Options:</li> <li>\"columns\": {column -&gt; {index -&gt; value}}</li> <li>\"index\": {index -&gt; {column -&gt; value}}</li> <li>\"records\": [{column -&gt; value}, ... , {column -&gt; value}]</li> <li>\"list\": {column -&gt; [values]}</li> <li>\"split\": Dict with keys: index, columns, and data</li> </ul> <ul> <li>Dictionary representation of the DataFrame.</li> </ul> Source code in <code>intellikit/common.py</code> <pre><code>def dataframe_to_dict(df, orientation=\"columns\"):\n    \"\"\"\n    Convert a DataFrame to a dictionary in various orientations.\n\n    Parameters:\n    - df: The DataFrame to convert.\n    - orientation: The dictionary orientation.\n      Options:\n      - \"columns\": {column -&gt; {index -&gt; value}}\n      - \"index\": {index -&gt; {column -&gt; value}}\n      - \"records\": [{column -&gt; value}, ... , {column -&gt; value}]\n      - \"list\": {column -&gt; [values]}\n      - \"split\": Dict with keys: index, columns, and data\n\n    Returns:\n    - Dictionary representation of the DataFrame.\n    \"\"\"\n    # Supported orientations for to_dict() method\n    valid_orientations = {\"dict\", \"index\", \"records\", \"list\", \"split\"}\n\n    # Check if the given orientation is valid\n    if orientation not in valid_orientations:\n        raise ValueError(f\"Invalid orientation. Choose from {', '.join(valid_orientations)}\")\n\n    # Convert DataFrame to dictionary\n    return df.to_dict(orient=orientation)\n</code></pre>"},{"location":"common/#intellikit.common.linearRetriever","title":"<code>linearRetriever(df, query, similarity_functions, feature_weights, top_n=1)</code>","text":"<p>linear retriever performs a K-NN search by computing all similarities between the query and each case one by one sequentially</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>DataFrame containing features for case characterization</p> required <code>similarity_function</code> <code>Dictionary</code> <p>A dictionary containing similarity functions for each feature in the casebase.</p> required <code>query</code> <code>pandas.DataFrame</code> <p>Datathe total similarity or weighted similarity column</p> required <code>feature_weights</code> <code>Dictionary</code> <p>A dictionary of weights assigned to each feature</p> required <code>top_n</code> <code>int</code> <p>Number of top similar cases to return.</p> <code>1</code> <p>Returns:</p> Type Description <code>dataframe</code> <p>top k cases from the casebase.</p> Source code in <code>intellikit/common.py</code> <pre><code>def linearRetriever(df, query, similarity_functions, feature_weights, top_n=1):\n    \"\"\"\n  linear retriever performs a K-NN search by computing all similarities between the query and each case one by one sequentially\n\n\n  Args:\n      df (pandas.DataFrame): DataFrame containing features for case characterization\n      similarity_function (Dictionary): A dictionary containing similarity functions for each feature in the casebase.\n      query (pandas.DataFrame): Datathe total similarity or weighted similarity column\n      feature_weights (Dictionary): A dictionary of weights assigned to each feature\n      top_n (int): Number of top similar cases to return.\n\n  Returns:\n      dataframe:  top k cases from the casebase.\n  \"\"\"\n    # Create a DataFrame to store similarities\n    similarities = pd.DataFrame(index=df.index)\n\n    # Iterate over the columns in the DataFrame\n    for feature in df.columns:\n        # Check if the feature is in the similarity_functions dictionary\n        if feature in similarity_functions:\n            # Retrieve the similarity function for the feature\n            similarity_function = similarity_functions[feature]\n\n            # Check if feature weight is iterable, convert to float if necessary\n            feature_weight = float(feature_weights.get(feature, 1.0))  # Default to 1.0 if not provided\n\n            # Apply the similarity function to calculate similarities\n            similarities[feature] = similarity_function(df[[feature]].copy(), query[[feature]], feature) * feature_weight\n        else:\n            # If the feature is not found in the similarity_functions dictionary, set the similarity to 0\n            similarities[feature] = 0.0\n\n    # Calculate total similarity as the sum of weighted similarities\n    similarities['total_similarity'] = similarities.sum(axis=1)\n\n    # Select top N cases with the lowest total similarity\n    top_n_indices = similarities['total_similarity'].nlargest(top_n).index\n    top_n_cases = df.loc[top_n_indices]\n\n    return top_n_cases\n</code></pre>"},{"location":"common/#intellikit.common.macfacRetriever","title":"<code>macfacRetriever(df, query, mac_features, fac_features, similarity_functions, feature_weights, top_n_mac=2, top_n_fac=1)</code>","text":"<p>MACFAC Retriever performs a two-staged retrieval where the first phase (MAC) uses a lightweight similarity to remove irrelevant cases for the second phase (FAC) where the final similarity for the filtered cases is evaluated.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>DataFrame containing features for case characterization</p> required <code>similarity_function</code> <code>Dictionary</code> <p>A dictionary containing similarity functions for each feature in the casebase.</p> required <code>query</code> <code>pandas.DataFrame</code> <p>Datathe total similarity or weighted similarity column</p> required <code>feature_weights</code> <code>Dictionary</code> <p>A dictionary of weights assigned to each feature</p> required <code>mac_features</code> <p>A list of features to use for the MAC phase (mac_features = ['feature4', 'feature5'])</p> required <code>fac_features</code> <p>A list of features to use for the FAC phase</p> required <code>top_n_mac</code> <code>int</code> <p>Number of top similar cases to return during the MAC phase.</p> <code>2</code> <code>top_n_fac</code> <code>int</code> <p>Number of top similar cases to return during the FAC phase</p> <code>1</code> <p>Returns:</p> Type Description <code>dataframe</code> <p>top k cases from the casebase specified for the FAC phase.</p> Source code in <code>intellikit/common.py</code> <pre><code>def macfacRetriever(df, query, mac_features, fac_features, similarity_functions, feature_weights, top_n_mac=2, top_n_fac=1):\n    \"\"\"\n  MACFAC Retriever performs a two-staged retrieval where the first phase (MAC) uses a lightweight similarity to remove irrelevant cases for the second phase (FAC) where the final similarity for the filtered cases is evaluated.\n\n  Args:\n      df (pandas.DataFrame): DataFrame containing features for case characterization\n      similarity_function (Dictionary): A dictionary containing similarity functions for each feature in the casebase.\n      query (pandas.DataFrame): Datathe total similarity or weighted similarity column\n      feature_weights (Dictionary): A dictionary of weights assigned to each feature\n      mac_features: A list of features to use for the MAC phase (mac_features = ['feature4', 'feature5'])\n      fac_features: A list of features to use for the FAC phase\n      top_n_mac (int): Number of top similar cases to return during the MAC phase.\n      top_n_fac (int): Number of top similar cases to return during the FAC phase\n\n  Returns:\n      dataframe:  top k cases from the casebase specified for the FAC phase.\n  \"\"\"\n    filtered_df = mac_stage(df, query, mac_features, similarity_functions, top_n_mac)\n    top_similar_cases = fac_stage(filtered_df, query, fac_features, similarity_functions, feature_weights, top_n_fac)\n    return top_similar_cases\n</code></pre>"},{"location":"common/#intellikit.common.parallelRetriever","title":"<code>parallelRetriever(df, query, similarity_functions, feature_weights, top_n=1)</code>","text":"<p>Parallel linear retriever performs a K-NN search by computing all similarities between the query and each case using all available computing cors of the respective CPU.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>DataFrame containing features for case characterization</p> required <code>similarity_function</code> <code>Dictionary</code> <p>A dictionary containing similarity functions for each feature in the casebase.</p> required <code>query</code> <code>pandas.DataFrame</code> <p>Datathe total similarity or weighted similarity column</p> required <code>feature_weights</code> <code>Dictionary</code> <p>A dictionary of weights assigned to each feature</p> required <code>top_n</code> <code>int</code> <p>Number of top similar cases to return.</p> <code>1</code> <p>Returns:</p> Type Description <code>dataframe</code> <p>top k cases from the casebase.</p> Source code in <code>intellikit/common.py</code> <pre><code>def parallelRetriever(df, query, similarity_functions, feature_weights, top_n=1):\n    \"\"\"\n  Parallel linear retriever performs a K-NN search by computing all similarities between the query and each case using all available computing cors of the respective CPU.\n\n  Args:\n      df (pandas.DataFrame): DataFrame containing features for case characterization\n      similarity_function (Dictionary): A dictionary containing similarity functions for each feature in the casebase.\n      query (pandas.DataFrame): Datathe total similarity or weighted similarity column\n      feature_weights (Dictionary): A dictionary of weights assigned to each feature\n      top_n (int): Number of top similar cases to return.\n\n  Returns:\n      dataframe:  top k cases from the casebase.\n  \"\"\"\n    with Pool(cpu_count()) as pool:\n        # Use starmap to pass additional arguments to calculate_similarity\n        similarity_results = pool.starmap(calculate_similarity, [(feature, df, query, similarity_functions, feature_weights) for feature in df.columns])\n\n    similarities = pd.concat(similarity_results, axis=1)\n    similarities['total_similarity'] = similarities.sum(axis=1)\n\n    # Select top N cases with the lowest total similarity\n    top_n_indices = similarities['total_similarity'].nlargest(top_n).index\n    top_n_cases = df.loc[top_n_indices]\n\n    return top_n_cases\n</code></pre>"},{"location":"common/#intellikit.common.retrieve_topk","title":"<code>retrieve_topk(cases, similarity_data, sim_column, k)</code>","text":"<p>Ranks features in a DataFrame by total similarity and returns top k results.</p> <p>Parameters:</p> Name Type Description Default <code>cases</code> <code>pandas.DataFrame</code> <p>DataFrame containing features</p> required <code>similarity_data(pandas.DataFrame)</code> <p>DataFrame containing similarity scores for each feature and a 'total_similarity' column.</p> required <code>sim_column</code> <p>the total similarity or weighted similarity column</p> required <code>k</code> <code>int</code> <p>Number of top features to return.</p> required <p>Returns:</p> Type Description <code>dataframe</code> <p>top k cases from the casebase.</p> Source code in <code>intellikit/common.py</code> <pre><code>def retrieve_topk(cases, similarity_data, sim_column, k):\n  \"\"\"\n  Ranks features in a DataFrame by total similarity and returns top k results.\n\n  Args:\n      cases (pandas.DataFrame): DataFrame containing features\n      similarity_data(pandas.DataFrame): DataFrame containing similarity scores for each feature and a 'total_similarity' column.\n      sim_column: the total similarity or weighted similarity column\n      k (int): Number of top features to return.\n\n  Returns:\n      dataframe:  top k cases from the casebase.\n  \"\"\"\n\n  #Combining the sets\n  merged = pd.concat([cases, similarity_data], axis=1)\n\n  # Sort DataFrame by 'total_similarity' in descending order\n  sorting_df = merged.sort_values(by=sim_column, ascending=False)\n\n  # Get the column to keep from the second DataFrame (assuming there's only one)\n  column_to_keep = similarity_data.filter(like=sim_column).columns[0]  # Extract column name containing 'F'\n  # Keep only the desired columns using list comprehension\n  desired_columns = [col for col in merged.columns if col in cases.columns or col == column_to_keep]\n  result_df = sorting_df[desired_columns]\n\n\n  # Select top k features\n  top_k_cases = result_df.head(k)\n\n  return top_k_cases\n</code></pre>"},{"location":"common/#intellikit.common.stream_text","title":"<code>stream_text(text, delay=0.02)</code>","text":"<p>Printing text with a slight delay like ChatGPT</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>plain text</code> <p>The text to be printed with a slight delay for example a paragraph or document</p> required <code>delay</code> <code>float</code> <p>Amount of time to delay. Defaults to 0.02.</p> <code>0.02</code> Source code in <code>intellikit/common.py</code> <pre><code>def stream_text(text, delay=0.02):\n    \"\"\"\n    Printing text with a slight delay like ChatGPT\n\n    Args:\n        text (plain text): The text to be printed with a slight delay for example a paragraph or document\n        delay (float, optional): Amount of time to delay. Defaults to 0.02.\n    \"\"\"\n    for char in text:\n        print(char, end = \"\", flush = True)\n        time.sleep(delay)\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/ArthurKakande/intellikit/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>intellikit could always use more documentation, whether as part of the official intellikit docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/ArthurKakande/intellikit/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up intellikit for local development.</p> <ol> <li> <p>Fork the intellikit repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/intellikit.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv intellikit\n$ cd intellikit/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 intellikit tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/ArthurKakande/intellikit/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install intellikit, run this command in your terminal:</p> <pre><code>pip install intellikit\n</code></pre> <p>This is the preferred method to install intellikit, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install intellikit from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/ArthurKakande/intellikit\n</code></pre>"},{"location":"intellikit/","title":"intellikit module","text":"<p>Main module.</p>"},{"location":"intellikit/#intellikit.intellikit.calculate_word_similarity","title":"<code>calculate_word_similarity(df, query, text_columns)</code>","text":"<p>Calculate the similarity score between the query and the case. This basic function returns a value of 1 for all words that match and a value of zero when the words differ</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>dataframme, required</code> <p>The dataframme containing the cases</p> required <code>query</code> <code>dictionary, required</code> <p>The search query corresponding to a case you are looking for.</p> required <code>text_columns</code> <code>list, required</code> <p>The columns from the dataframme for which to compare with the query values.</p> required <p>Returns:</p> Type Description <code>dataframme</code> <p>The dataframme of the calculated similarity values for each case against the query. </p> Source code in <code>intellikit/intellikit.py</code> <pre><code>def calculate_word_similarity(df, query, text_columns):\n    \"\"\"Calculate the similarity score between the query and the case. This basic function returns a value of 1 for all words that match and a value of zero when the words differ\n\n    Args:\n        df (dataframme, required): The dataframme containing the cases\n        query (dictionary, required): The search query corresponding to a case you are looking for.\n        text_columns (list, required): The columns from the dataframme for which to compare with the query values.\n\n    Returns:\n        dataframme: The dataframme of the calculated similarity values for each case against the query. \n    \"\"\"    \n    for column in text_columns:\n        # Check for NaN values\n        df[column] = df[column].fillna('')\n        query[column] = query[column].fillna('')\n\n        # Ensure the column is of string type\n        if df[column].dtype != 'object':\n            df[column] = df[column].astype(str)\n\n        if query[column].dtype != 'object':\n            query[column] = query[column].astype(str)\n\n        # Check if documents are not empty or too short\n        if df[column].str.len().sum() == 0 or query[column].str.len().sum() == 0:\n            df[column] = 0  # Set similarity to 0 if documents are empty or too short\n        else:\n            # Compare words and set similarity accordingly\n            df[column] = df[column].apply(lambda x: 1 if x == query[column].iloc[0] else 0)\n\n    return df\n</code></pre>"},{"location":"ir/","title":"IR module","text":"<p>Document Retrieval module.</p>"},{"location":"ir/#intellikit.ir.bm25","title":"<code>bm25(query, documents, k=5, b=0.75, k1=1.5)</code>","text":"<p>Implement BM25 algorithm to retrieve top k similar documents to a query.</p> <p>query (str): The query string. documents (list): A list of document strings. k (int, optional): The number of top similar documents to retrieve. Defaults to 5. b (float, optional): The parameter controlling the impact of document length on BM25 score. Defaults to 0.75. k1 (float, optional): The parameter controlling the impact of term frequency on BM25 score. Defaults to 1.5.</p> Source code in <code>intellikit/ir.py</code> <pre><code>def bm25(query, documents, k=5, b=0.75, k1=1.5):\n    \"\"\"\n    Implement BM25 algorithm to retrieve top k similar documents to a query.\n\n    Parameters:\n    query (str): The query string.\n    documents (list): A list of document strings.\n    k (int, optional): The number of top similar documents to retrieve. Defaults to 5.\n    b (float, optional): The parameter controlling the impact of document length on BM25 score. Defaults to 0.75.\n    k1 (float, optional): The parameter controlling the impact of term frequency on BM25 score. Defaults to 1.5.\n\n    Returns:\n    list: A list of tuples containing the top k similar documents and their corresponding BM25 scores.\n    \"\"\"\n    # Tokenize query and documents\n    query_tokens = query.lower().split()\n    document_tokens = [doc.lower().split() for doc in documents]\n\n    # Create vocabulary\n    vocabulary = list(set(query_tokens))\n    for doc_tokens in document_tokens:\n        vocabulary.extend(doc_tokens)\n    vocabulary = list(set(vocabulary))\n\n    # Calculate document lengths\n    doc_lengths = np.array([len(doc_tokens) for doc_tokens in document_tokens])\n\n    # Create term frequency (TF) matrix\n    tf_matrix = np.zeros((len(documents), len(vocabulary)))\n    for i, doc_tokens in enumerate(document_tokens):\n        for token in doc_tokens:\n            tf_matrix[i, vocabulary.index(token)] += 1\n\n    # Calculate document frequency (DF) vector\n    df_vector = np.zeros(len(vocabulary))\n    for token in query_tokens:\n        df_vector[vocabulary.index(token)] += 1\n\n    # Calculate inverse document frequency (IDF) vector\n    idf_vector = np.log((len(documents) - df_vector + 0.5) / (df_vector + 0.5))\n\n    # Calculate average document length\n    avg_doc_length = np.mean(doc_lengths)\n\n    # Calculate BM25 scores\n    scores = []\n    for i in range(len(documents)):\n        tf = tf_matrix[i]\n        doc_length = doc_lengths[i]\n        score = np.sum(idf_vector * tf * (k1 + 1) / (tf + k1 * (1 - b + b * doc_length / avg_doc_length)))\n        scores.append((i, score))\n\n    # Sort documents by BM25 score\n    scores.sort(key=lambda x: x[1], reverse=True)\n\n    # Return top k similar documents\n    top_k_similar_docs = []\n    for i in range(min(k, len(scores))):\n        doc_index = scores[i][0]\n        top_k_similar_docs.append((documents[doc_index], scores[i][1]))\n\n    return top_k_similar_docs\n</code></pre>"},{"location":"ir/#intellikit.ir.cosine_similarity","title":"<code>cosine_similarity(v1, v2)</code>","text":"<p>Compute the cosine similarity between two vectors.</p> <p>v1 (numpy.ndarray): The first vector. v2 (numpy.ndarray): The second vector.</p> Source code in <code>intellikit/ir.py</code> <pre><code>def cosine_similarity(v1, v2):\n    \"\"\"\n    Compute the cosine similarity between two vectors.\n\n    Parameters:\n    v1 (numpy.ndarray): The first vector.\n    v2 (numpy.ndarray): The second vector.\n\n    Returns:\n    float: The cosine similarity between the two vectors.\n    \"\"\"\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    return dot_product / (norm_v1 * norm_v2)\n</code></pre>"},{"location":"ir/#intellikit.ir.sentence_transformers_retrieval","title":"<code>sentence_transformers_retrieval(query, documents, k=5, model_name='paraphrase-MiniLM-L6-v2')</code>","text":"<p>Apply sentence transformers to retrieve top k similar documents.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>text</code> <p>A search query for a document.</p> required <code>documents</code> <code>list</code> <p>A list of documents for the search.</p> required <code>k</code> <code>int</code> <p>Number of top results to return. Defaults to 5.</p> <code>5</code> <code>model_name</code> <code>str</code> <p>Open source sentence model to use. Defaults to 'paraphrase-MiniLM-L6-v2'.</p> <code>'paraphrase-MiniLM-L6-v2'</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of documents relevant to the search query.</p> Source code in <code>intellikit/ir.py</code> <pre><code>def sentence_transformers_retrieval(query, documents, k=5, model_name='paraphrase-MiniLM-L6-v2'):\n    \"\"\"\n    Apply sentence transformers to retrieve top k similar documents.\n\n    Args:\n        query (text): A search query for a document.\n        documents (list): A list of documents for the search.\n        k (int, optional): Number of top results to return. Defaults to 5.\n        model_name (str, optional): Open source sentence model to use. Defaults to 'paraphrase-MiniLM-L6-v2'.\n\n    Returns:\n        list: A list of documents relevant to the search query.\n    \"\"\"\n    # Load pre-trained sentence transformer model. Specify the model using model_name\n    model = SentenceTransformer(model_name)\n\n    # Encode query and documents into embeddings\n    query_embedding = model.encode([query])[0]\n    document_embeddings = model.encode(documents)\n\n    # Calculate cosine similarity between query and documents\n    similarities = [cosine_similarity(query_embedding, doc_embedding) for doc_embedding in document_embeddings]\n\n    # Sort documents by similarity score\n    similarities_with_indices = list(enumerate(similarities))\n    similarities_with_indices.sort(key=lambda x: x[1], reverse=True)\n\n    # Return top k similar documents\n    top_k_similar_docs = []\n    for i in range(min(k, len(similarities_with_indices))):\n        doc_index = similarities_with_indices[i][0]\n        top_k_similar_docs.append((documents[doc_index], similarities_with_indices[i][1]))\n\n    return top_k_similar_docs\n</code></pre>"},{"location":"ir/#intellikit.ir.vector_space_model","title":"<code>vector_space_model(query, documents, k=5)</code>","text":"<p>Calculates the top k similar documents to a given query using the Vector Space Model.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query string.</p> required <code>documents</code> <code>list</code> <p>A list of document strings.</p> required <code>k</code> <code>int</code> <p>The number of similar documents to return. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of tuples containing the top k similar documents and their similarity scores.</p> Source code in <code>intellikit/ir.py</code> <pre><code>def vector_space_model(query, documents, k=5):\n    \"\"\"\n    Calculates the top k similar documents to a given query using the Vector Space Model.\n\n    Args:\n        query (str): The query string.\n        documents (list): A list of document strings.\n        k (int, optional): The number of similar documents to return. Defaults to 5.\n\n    Returns:\n        list: A list of tuples containing the top k similar documents and their similarity scores.\n    \"\"\"\n    # Tokenize query and documents\n    query_tokens = query.lower().split()\n    document_tokens = [doc.lower().split() for doc in documents]\n\n    # Create vocabulary\n    vocabulary = list(set(query_tokens))\n    for doc_tokens in document_tokens:\n        vocabulary.extend(doc_tokens)\n    vocabulary = list(set(vocabulary))\n\n    # Create term frequency (TF) matrix\n    tf_matrix = np.zeros((len(documents), len(vocabulary)))\n    for i, doc_tokens in enumerate(document_tokens):\n        for token in doc_tokens:\n            tf_matrix[i, vocabulary.index(token)] += 1\n\n    # Create document frequency (DF) vector\n    df_vector = np.zeros(len(vocabulary))\n    for token in query_tokens:\n        df_vector[vocabulary.index(token)] += 1\n\n    # Calculate inverse document frequency (IDF) vector\n    idf_vector = np.log(len(documents) / (df_vector + 1))\n\n    # Calculate TF-IDF matrix\n    tfidf_matrix = tf_matrix * idf_vector\n\n    # Calculate query vector\n    query_vector = np.zeros(len(vocabulary))\n    for token in query_tokens:\n        if token in vocabulary:\n            query_vector[vocabulary.index(token)] += 1\n    query_vector *= idf_vector\n\n    # Calculate cosine similarity between query vector and document vectors\n    similarities = []\n    for i in range(len(documents)):\n        sim = cosine_similarity(query_vector, tfidf_matrix[i])\n        similarities.append((i, sim))\n\n    # Sort documents by similarity score\n    similarities.sort(key=lambda x: x[1], reverse=True)\n\n    # Return top k similar documents\n    top_k_similar_docs = []\n    for i in range(min(k, len(similarities))):\n        doc_index = similarities[i][0]\n        top_k_similar_docs.append((documents[doc_index], similarities[i][1]))\n\n    return top_k_similar_docs\n</code></pre>"},{"location":"sim/","title":"Sim module","text":"<p>Sim module.</p>"},{"location":"sim/#intellikit.sim.case_higher_than_query_similarity","title":"<code>case_higher_than_query_similarity(query, case)</code>","text":"<p>Checks if a case value is higher than the query value and returns a similarity score.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>_type_</code> <p>The query value.</p> required <code>case</code> <code>_type_</code> <p>The case value.</p> required <p>Returns:</p> Type Description <code>float</code> <p>A similarity score of 0 if the case value is higher than the query value, and 1 otherwise.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def case_higher_than_query_similarity(query, case):\n    \"\"\"Checks if a case value is higher than the query value and returns a similarity score.\n\n    Args:\n        query (_type_): The query value.\n        case (_type_): The case value.\n\n    Returns:\n        float: A similarity score of 0 if the case value is higher than the query value, and 1 otherwise.\n    \"\"\"\n    if case &gt; query:\n        return 0.0\n    else:\n        return 1.0\n</code></pre>"},{"location":"sim/#intellikit.sim.check_string_em","title":"<code>check_string_em(str1, str2)</code>","text":"<p>Check if two strings are an exact match (case-insensitive) and return similarity score.</p> <p>Parameters:</p> Name Type Description Default <code>str1</code> <p>The first string.</p> required <code>str2</code> <p>The second string.</p> required <p>Returns:</p> Type Description <code>float</code> <p>1.0 if the strings are an exact match (case-insensitive), 0.0 otherwise.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def check_string_em(str1, str2):\n    \"\"\"\n    Check if two strings are an exact match (case-insensitive) and return similarity score.\n\n    Args:\n        str1: The first string.\n        str2: The second string.\n\n    Returns:\n        float: 1.0 if the strings are an exact match (case-insensitive), 0.0 otherwise.\n    \"\"\"\n    if str1.strip().lower() == str2.strip().lower():\n        return 1.0\n    else:\n        return 0.0\n</code></pre>"},{"location":"sim/#intellikit.sim.dis_levenshtein","title":"<code>dis_levenshtein(df, query, feature)</code>","text":"<p>Calculate the Levenshtein distance between the query value and each value in the specified feature column of a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame containing the feature column.</p> required <code>query</code> <code>DataFrame</code> <p>The DataFrame containing the query value.</p> required <code>feature</code> <code>str</code> <p>The name of the feature column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with the Levenshtein distances between the query value and each value in the feature column.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def dis_levenshtein(df, query, feature):\n    \"\"\"\n    Calculate the Levenshtein distance between the query value and each value in the specified feature column of a DataFrame.\n\n    Args:\n        df (DataFrame): The DataFrame containing the feature column.\n        query (DataFrame): The DataFrame containing the query value.\n        feature (str): The name of the feature column.\n\n    Returns:\n        DataFrame: A DataFrame with the Levenshtein distances between the query value and each value in the feature column.\n    \"\"\"\n    # Get the query value for the feature\n    query_value = query[feature].iloc[0]\n\n    # Calculate Levenshtein distance between query value and each value in the feature column\n    levenshtein_distances = df[feature].apply(lambda x: levenshtein_distance(x, query_value))\n\n    # Convert the Series to a DataFrame column with the feature name retained\n    df[feature] = pd.DataFrame(levenshtein_distances, columns=[feature])\n\n    return df[feature]\n</code></pre>"},{"location":"sim/#intellikit.sim.level_similarity","title":"<code>level_similarity(level1, level2)</code>","text":"<p>Calculates the similarity score between two levels (small, medium, large).</p> <p>Parameters:</p> Name Type Description Default <code>level1</code> <p>The first level string (e.g., \"small\").</p> required <code>level2</code> <p>The second level string (e.g., \"medium\").</p> required <p>Returns:</p> Type Description <p>A similarity score between 0 and 1. (returns 1 if level1=level2, 0.5 if the level1 is close to level2)</p> Source code in <code>intellikit/sim.py</code> <pre><code>def level_similarity(level1, level2):\n  \"\"\"\n  Calculates the similarity score between two levels (small, medium, large).\n\n  Args:\n      level1: The first level string (e.g., \"small\").\n      level2: The second level string (e.g., \"medium\").\n\n  Returns:\n      A similarity score between 0 and 1. (returns 1 if level1=level2, 0.5 if the level1 is close to level2)\n  \"\"\"\n  options = [\"small\", \"medium\", \"large\"]\n  distance = abs(options.index(level1) - options.index(level2))\n  max_distance = len(options) - 1\n  if level1 == level2:\n    return 1\n  elif distance == 1:\n    return 0.5\n  else:\n    return 0\n</code></pre>"},{"location":"sim/#intellikit.sim.levenshtein_distance","title":"<code>levenshtein_distance(str1, str2)</code>","text":"<p>Calculate the Levenshtein distance between two strings.</p> <p>The Levenshtein distance is a measure of the difference between two strings. It is defined as the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.</p> <p>Parameters:</p> Name Type Description Default <code>str1</code> <code>str</code> <p>The first string.</p> required <code>str2</code> <code>str</code> <p>The second string.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The Levenshtein distance between the two strings.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def levenshtein_distance(str1, str2):\n    \"\"\"\n    Calculate the Levenshtein distance between two strings.\n\n    The Levenshtein distance is a measure of the difference between two strings.\n    It is defined as the minimum number of single-character edits (insertions,\n    deletions, or substitutions) required to change one string into the other.\n\n    Args:\n        str1 (str): The first string.\n        str2 (str): The second string.\n\n    Returns:\n        int: The Levenshtein distance between the two strings.\n    \"\"\"\n    # Initialize a matrix where dp[i][j] represents the distance between\n    # the first i characters of str1 and the first j characters of str2.\n    dp = [[0] * (len(str2) + 1) for _ in range(len(str1) + 1)]\n\n    # Set up the initial distances when one of the strings is empty.\n    for i in range(len(str1) + 1):\n        dp[i][0] = i\n    for j in range(len(str2) + 1):\n        dp[0][j] = j\n\n    # Compute the distances.\n    for i in range(1, len(str1) + 1):\n        for j in range(1, len(str2) + 1):\n            if str1[i - 1] == str2[j - 1]:  # No change needed if characters are the same.\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                # Calculate costs for substitution, insertion, and deletion.\n                substitution_cost = dp[i - 1][j - 1] + 1\n                insertion_cost = dp[i][j - 1] + 1\n                deletion_cost = dp[i - 1][j] + 1\n                # Find the minimum of these three options.\n                dp[i][j] = min(substitution_cost, insertion_cost, deletion_cost)\n\n    # The bottom-right corner of the matrix contains the final Levenshtein distance.\n    return dp[-1][-1]\n</code></pre>"},{"location":"sim/#intellikit.sim.log_similarity","title":"<code>log_similarity(query, case)</code>","text":"<p>Calculate similarity score based on the log values (base 10) of two numeric values.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>The query numeric value.</p> required <code>case</code> <p>The case numeric value.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0 and 1.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def log_similarity(query, case):\n    \"\"\"\n    Calculate similarity score based on the log values (base 10) of two numeric values.\n\n    Args:\n        query: The query numeric value.\n        case: The case numeric value.\n\n    Returns:\n        float: Similarity score between 0 and 1.\n    \"\"\"\n    # Convert values to their logarithmic values (base 10)\n    log_query = math.log10(query)\n    log_case = math.log10(case)\n\n    # Calculate the absolute difference between the log values\n    distance = abs(log_query - log_case)\n\n    # Convert the distance to a similarity score between 0 and 1\n    # Here we assume a maximum possible distance for normalization, for instance, log10(max_value) - log10(min_value)\n    # If you know the expected range of your values, you can use that for better normalization\n    max_distance = math.log10(10**6)  # Example max range for normalization\n    similarity_score = max(0, 1 - distance / max_distance)\n\n    return similarity_score\n</code></pre>"},{"location":"sim/#intellikit.sim.normalized_hamming_distance","title":"<code>normalized_hamming_distance(str1, str2)</code>","text":"<p>Calculates the normalized Hamming distance between two strings.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def normalized_hamming_distance(str1, str2):\n  \"\"\"Calculates the normalized Hamming distance between two strings.\"\"\"\n  ham_dist = hamming_distance(str1, str2)\n  # Get the maximum length of the strings\n  max_len = max(len(str1), len(str2))\n  # Normalize by the maximum length\n  ham_sim = 1 - (ham_dist / max_len)\n  return ham_sim\n</code></pre>"},{"location":"sim/#intellikit.sim.normalized_levenshtein_distance","title":"<code>normalized_levenshtein_distance(str1, str2)</code>","text":"<p>Calculates the normalized Levenshtein distance between two strings.</p> <p>str1 (str): The first string. str2 (str): The second string.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def normalized_levenshtein_distance(str1, str2):\n    \"\"\"\n    Calculates the normalized Levenshtein distance between two strings.\n\n    Parameters:\n    str1 (str): The first string.\n    str2 (str): The second string.\n\n    Returns:\n    float: The normalized Levenshtein distance between the two strings.\n    \"\"\"\n    # Get the Levenshtein distance\n    lev_distance = levenshtein_distance(str1, str2)\n    # Get the length of the longer string\n    max_len = max(len(str1), len(str2))\n    # Normalize by the maximum length\n    lev_sim = 1 - (lev_distance / max_len)\n    return lev_sim\n</code></pre>"},{"location":"sim/#intellikit.sim.query_exact_match","title":"<code>query_exact_match(query, case)</code>","text":"<p>Check if the query value is an exact match with the case value and return a similarity score.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>_type_</code> <p>The query value.</p> required <code>case</code> <code>_type_</code> <p>The case value.</p> required <p>Returns:</p> Type Description <code>_type_</code> <p>A similarity score of 1.0 if the query value is an exact match with the case value, otherwise returns 0.0.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def query_exact_match(query, case):\n    \"\"\"Check if the query value is an exact match with the case value and return a similarity score.\n\n    Args:\n        query (_type_): The query value.\n        case (_type_): The case value.\n\n    Returns:\n        _type_: A similarity score of 1.0 if the query value is an exact match with the case value, otherwise returns 0.0.\n    \"\"\"\n    if query == case:\n        return 1.0\n    else:\n        return 0.0\n</code></pre>"},{"location":"sim/#intellikit.sim.query_higher_than_case_similarity","title":"<code>query_higher_than_case_similarity(query, case)</code>","text":"<p>Check if the query is higher than the case similarity.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>float</code> <p>The similarity score of the query.</p> required <code>case</code> <code>float</code> <p>The similarity score of the case.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Returns 0.0 if the query is higher than the case similarity, otherwise returns 1.0.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def query_higher_than_case_similarity(query, case):\n    \"\"\"Check if the query is higher than the case similarity.\n\n    Args:\n        query (float): The similarity score of the query.\n        case (float): The similarity score of the case.\n\n    Returns:\n        float: Returns 0.0 if the query is higher than the case similarity, otherwise returns 1.0.\n    \"\"\"\n    if query &gt; case:\n        return 0.0\n    else:\n        return 1.0\n</code></pre>"},{"location":"sim/#intellikit.sim.sent_cosine_similarity","title":"<code>sent_cosine_similarity(sentence1, sentence2)</code>","text":"<p>Calculates the cosine similarity between two sentences.</p> <p>This function takes in two sentences and calculates the cosine similarity between them.  The cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It is defined as the cosine of the angle between the two vectors.</p> <p>Parameters:</p> Name Type Description Default <code>sentence1</code> <code>str</code> <p>The first sentence.</p> required <code>sentence2</code> <code>str</code> <p>The second sentence.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The cosine similarity score between the two sentences. The score is between 0 and 1,         where 0 indicates no similarity and 1 indicates identical sentences.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def sent_cosine_similarity(sentence1, sentence2):\n    \"\"\"Calculates the cosine similarity between two sentences.\n\n    This function takes in two sentences and calculates the cosine similarity between them. \n    The cosine similarity is a measure of similarity between two non-zero vectors of an inner product space.\n    It is defined as the cosine of the angle between the two vectors.\n\n    Args:\n        sentence1 (str): The first sentence.\n        sentence2 (str): The second sentence.\n\n    Returns:\n        float: The cosine similarity score between the two sentences. The score is between 0 and 1, \n               where 0 indicates no similarity and 1 indicates identical sentences.\n    \"\"\"\n    # Convert sentences to lowercase and split into words\n    words1 = sentence1.lower().split()\n    words2 = sentence2.lower().split()\n\n    # Build a vocabulary of unique words from both sentences\n    unique_words = set(words1).union(set(words2))\n\n    # Create frequency vectors for each sentence based on the vocabulary\n    freq_vector1 = []\n    freq_vector2 = []\n\n    for word in unique_words:\n        freq_vector1.append(words1.count(word))\n        freq_vector2.append(words2.count(word))\n\n    # Calculate the dot product of the two vectors\n    dot_product = sum(f1 * f2 for f1, f2 in zip(freq_vector1, freq_vector2))\n\n    # Calculate the magnitude of each vector\n    magnitude1 = math.sqrt(sum(f ** 2 for f in freq_vector1))\n    magnitude2 = math.sqrt(sum(f ** 2 for f in freq_vector2))\n\n    # Handle the case when one of the magnitudes is zero (no overlap in words)\n    if magnitude1 == 0 or magnitude2 == 0:\n        return 0.0\n\n    # Calculate and return cosine similarity\n    return dot_product / (magnitude1 * magnitude2)\n</code></pre>"},{"location":"sim/#intellikit.sim.sim_CaseHigher","title":"<code>sim_CaseHigher(df, query, feature)</code>","text":"<p>If the case value is higher than the query value, the similarity will always be 0.0.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The case charactrization.</p> required <code>query</code> <p>The query being checked.</p> required <code>feature</code> <p>The specific feature.</p> required <p>Returns:</p> Type Description <p>A column containing the similarity scores.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def sim_CaseHigher(df, query, feature):\n    \"\"\"\n    If the case value is higher than the query value, the similarity will always be 0.0.\n\n    Args:\n        df: The case charactrization.\n        query: The query being checked.\n        feature: The specific feature.\n\n    Returns:\n        A column containing the similarity scores.\n    \"\"\"\n    # Get the query value for the feature\n    query_value = query[feature].iloc[0]\n\n    # Calculate \"case higher\" distance between query value and each value in the feature column\n    ch_distances = df[feature].apply(lambda x: case_higher_than_query_similarity(x, query_value))\n\n    # Convert the Series to a DataFrame column with the feature name retained\n    df[feature] = pd.DataFrame(ch_distances, columns=[feature])\n\n    return df[feature]\n</code></pre>"},{"location":"sim/#intellikit.sim.sim_QueryHigher","title":"<code>sim_QueryHigher(df, query, feature)</code>","text":"<p>If the query value is higher than the case value, the similarity will always be 0.0.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The case charactrization.</p> required <code>query</code> <p>The query being checked.</p> required <code>feature</code> <p>The specific feature.</p> required <p>Returns:</p> Type Description <p>A column containing the similarities.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def sim_QueryHigher(df, query, feature):\n    \"\"\"\n    If the query value is higher than the case value, the similarity will always be 0.0.\n\n    Args:\n        df: The case charactrization.\n        query: The query being checked.\n        feature: The specific feature.\n\n    Returns:\n        A column containing the similarities.\n    \"\"\"\n    # Get the query value for the feature\n    query_value = query[feature].iloc[0]\n\n    # Calculate \"query higher\" distance between query value and each value in the feature column\n    qh_distances = df[feature].apply(lambda x: query_higher_than_case_similarity(x, query_value))\n\n    # Convert the Series to a DataFrame column with the feature name retained\n    df[feature] = pd.DataFrame(qh_distances, columns=[feature])\n\n    return df[feature]\n</code></pre>"},{"location":"sim/#intellikit.sim.sim_level","title":"<code>sim_level(df, query, feature)</code>","text":"<p>Calculate the level similarity (small, medium, large) between the query value and each value in the specified feature column of a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame containing the feature column.</p> required <code>query</code> <code>DataFrame</code> <p>The DataFrame containing the query value.</p> required <code>feature</code> <code>str</code> <p>The name of the feature column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame column with the level similarity values for each value in the feature column.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def sim_level(df, query, feature):\n    \"\"\"\n    Calculate the level similarity (small, medium, large) between the query value and each value in the specified feature column of a DataFrame.\n\n    Args:\n        df (DataFrame): The DataFrame containing the feature column.\n        query (DataFrame): The DataFrame containing the query value.\n        feature (str): The name of the feature column.\n\n    Returns:\n        DataFrame: A DataFrame column with the level similarity values for each value in the feature column.\n    \"\"\"\n    # Get the query value for the feature\n    query_value = query[feature].iloc[0]\n\n    # Calculate n-gram similarity between query value and each value in the feature column\n    level_similarities = df[feature].apply(lambda x: level_similarity(x, query_value))\n\n    # Convert the Series to a DataFrame column with the feature name retained\n    df[feature] = pd.DataFrame(level_similarities, columns=[feature])\n\n    return df[feature]\n</code></pre>"},{"location":"sim/#intellikit.sim.sim_levenshtein","title":"<code>sim_levenshtein(df, query, feature)</code>","text":"<p>Calculate the Levenshtein similarity between the query value and each value in the specified feature column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>query</code> <code>DataFrame</code> <p>The query DataFrame containing the value to compare.</p> required <code>feature</code> <code>str</code> <p>The name of the feature column to calculate the similarity for.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame column containing the Levenshtein similarity values for each value in the feature column.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def sim_levenshtein(df, query, feature):\n    \"\"\"Calculate the Levenshtein similarity between the query value and each value in the specified feature column.\n\n    Args:\n        df (DataFrame): The input DataFrame.\n        query (DataFrame): The query DataFrame containing the value to compare.\n        feature (str): The name of the feature column to calculate the similarity for.\n\n    Returns:\n        DataFrame: A DataFrame column containing the Levenshtein similarity values for each value in the feature column.\n    \"\"\"\n    # Get the query value for the feature\n    query_value = query[feature].iloc[0]\n\n    # Calculate Levenshtein distance between query value and each value in the feature column\n    levenshtein_similarities = df[feature].apply(lambda x: normalized_levenshtein_distance(x, query_value))\n\n    # Convert the Series to a DataFrame column with the feature name retained\n    df[feature] = pd.DataFrame(levenshtein_similarities, columns=[feature])\n\n    return df[feature]\n</code></pre>"},{"location":"sim/#intellikit.sim.sim_logDifference","title":"<code>sim_logDifference(df, query, feature)</code>","text":"<p>Calculate similarity score based on the log values (base 10) of a query value and a value from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The case charactrization.</p> required <code>query</code> <p>The query being checked.</p> required <code>feature</code> <p>The specific feature in the dataframe.</p> required <p>Returns:</p> Type Description <code>Dataframe column</code> <p>A column containing the similarities. </p> Source code in <code>intellikit/sim.py</code> <pre><code>def sim_logDifference(df, query, feature):\n    \"\"\"\n    Calculate similarity score based on the log values (base 10) of a query value and a value from the dataframe.\n\n    Args:\n        df: The case charactrization.\n        query: The query being checked.\n        feature: The specific feature in the dataframe.\n\n    Returns:\n        Dataframe column: A column containing the similarities. \n    \"\"\"\n    # Get the query value for the feature\n    query_value = query[feature].iloc[0]\n\n    # Calculate the \"exact match\" distance between query value and each value in the feature column\n    log_distances = df[feature].apply(lambda x: log_similarity(x, query_value))\n\n    # Convert the Series to a DataFrame column with the feature name retained\n    df[feature] = pd.DataFrame(log_distances, columns=[feature])\n\n    return df[feature]\n</code></pre>"},{"location":"sim/#intellikit.sim.sim_numEM","title":"<code>sim_numEM(df, query, feature)</code>","text":"<p>Check if the query and the case are an exact match. (Only works for numeric data type)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The case charactrization.</p> required <code>query</code> <p>The query being checked.</p> required <code>feature</code> <p>The specific feature.</p> required <p>Returns:</p> Type Description <p>A column with the similarities.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def sim_numEM(df, query, feature):\n    \"\"\"\n    Check if the query and the case are an exact match. (Only works for numeric data type)\n\n    Args:\n        df: The case charactrization.\n        query: The query being checked.\n        feature: The specific feature.\n\n    Returns:\n        A column with the similarities.\n    \"\"\"\n    # Get the query value for the feature\n    query_value = query[feature].iloc[0]\n\n    # Calculate the \"exact match\" distance between query value and each value in the feature column\n    em_distances = df[feature].apply(lambda x: query_exact_match(x, query_value))\n\n    # Convert the Series to a DataFrame column with the feature name retained\n    df[feature] = pd.DataFrame(em_distances, columns=[feature])\n\n    return df[feature]\n</code></pre>"},{"location":"sim/#intellikit.sim.sim_sentence_cosine","title":"<code>sim_sentence_cosine(df, query, feature)</code>","text":"<p>Calculate the sentence cosine similarity between a query sentence and a sentence from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the sentences.</p> required <code>query</code> <code>DataFrame</code> <p>The query sentence.</p> required <code>feature</code> <code>str</code> <p>The specific feature in the dataframe.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A column containing the sentence cosine similarities.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def sim_sentence_cosine(df, query, feature):\n    \"\"\"Calculate the sentence cosine similarity between a query sentence and a sentence from the dataframe.\n\n    Args:\n        df (DataFrame): The dataframe containing the sentences.\n        query (DataFrame): The query sentence.\n        feature (str): The specific feature in the dataframe.\n\n    Returns:\n        DataFrame: A column containing the sentence cosine similarities.\n    \"\"\"\n    # Get the query value for the feature\n    query_value = query[feature].iloc[0]\n\n    # Calculate Euclidean distance between query value and each value in the feature column\n    sent_cos_similarities = df[feature].apply(lambda x: sent_cosine_similarity(x, query_value))\n\n    # Convert the Series to a DataFrame column with the feature name retained\n    df[feature] = pd.DataFrame(sent_cos_similarities, columns=[feature])\n\n    return df[feature]\n</code></pre>"},{"location":"sim/#intellikit.sim.sim_stringEM","title":"<code>sim_stringEM(df, query, feature)</code>","text":"<p>Checks if two strings are an exact match (case-insensitive) and returns the similarity scores.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>The case charactrization.</p> required <code>query</code> <p>The query being checked.</p> required <code>feature</code> <p>The specific feature.</p> required <p>Returns:</p> Type Description <p>A column containing the similarity scores.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def sim_stringEM(df, query, feature):\n    \"\"\"\n    Checks if two strings are an exact match (case-insensitive) and returns the similarity scores.\n\n    Args:\n        df: The case charactrization.\n        query: The query being checked.\n        feature: The specific feature.\n\n    Returns:\n        A column containing the similarity scores.\n    \"\"\"\n    # Get the query value for the feature\n    query_value = query[feature].iloc[0]\n\n    # Calculate Levenshtein distance between query value and each value in the feature column\n    sem_similarities = df[feature].apply(lambda x: check_string_em(x, query_value))\n\n    # Convert the Series to a DataFrame column with the feature name retained\n    df[feature] = pd.DataFrame(sem_similarities, columns=[feature])\n\n    return df[feature]\n</code></pre>"},{"location":"sim/#intellikit.sim.sim_vector_cosine","title":"<code>sim_vector_cosine(df, query, feature)</code>","text":"<p>Calculate the cosine similarity between a query vector and each vector in a feature column of a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame containing the vector column.</p> required <code>query</code> <code>DataFrame</code> <p>The DataFrame containing the query vector.</p> required <code>feature</code> <code>str</code> <p>The name of the feature column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame column with the cosine similarity values between the query vector and each vector in the feature column.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def sim_vector_cosine(df, query, feature):\n    \"\"\"\n    Calculate the cosine similarity between a query vector and each vector in a feature column of a DataFrame.\n\n    Args:\n        df (DataFrame): The DataFrame containing the vector column.\n        query (DataFrame): The DataFrame containing the query vector.\n        feature (str): The name of the feature column.\n\n    Returns:\n        DataFrame: A DataFrame column with the cosine similarity values between the query vector and each vector in the feature column.\n    \"\"\"\n    # Get the query value for the feature\n    query_value = query[feature].iloc[0]\n\n    # Calculate cosine similarity between query value and each value in the feature column\n    vector_similarities = df[feature].apply(lambda x: vector_cosine_similarity(x, query_value))\n\n    # Convert the Series to a DataFrame column with the feature name retained\n    df[feature] = pd.DataFrame(vector_similarities, columns=[feature])\n\n    return df[feature]\n</code></pre>"},{"location":"sim/#intellikit.sim.similarity_time","title":"<code>similarity_time(user_time, opening_time, closing_time)</code>","text":"<p>Calculate the similarity between the user's time and the opening and closing times.</p> <p>Parameters:</p> Name Type Description Default <code>user_time</code> <code>str</code> <p>The time entered by the user in the format \"HH:MM\".</p> required <code>opening_time</code> <code>str</code> <p>The opening time in the format \"HH:MM\".</p> required <code>closing_time</code> <code>str</code> <p>The closing time in the format \"HH:MM\".</p> required <p>Returns:</p> Type Description <code>float</code> <p>The similarity score between the user's time and the opening and closing times.     - 1 if the user's time is within the opening and closing times and the difference is 4 hours or more.     - 0.5 if the user's time is within the opening and closing times and the difference is less than 4 hours.     - 0 if the user's time is outside the opening and closing times.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def similarity_time(user_time, opening_time, closing_time):\n    \"\"\"Calculate the similarity between the user's time and the opening and closing times.\n\n    Args:\n        user_time (str): The time entered by the user in the format \"HH:MM\".\n        opening_time (str): The opening time in the format \"HH:MM\".\n        closing_time (str): The closing time in the format \"HH:MM\".\n\n    Returns:\n        float: The similarity score between the user's time and the opening and closing times.\n            - 1 if the user's time is within the opening and closing times and the difference is 4 hours or more.\n            - 0.5 if the user's time is within the opening and closing times and the difference is less than 4 hours.\n            - 0 if the user's time is outside the opening and closing times.\n    \"\"\"\n    user_time = datetime.strptime(user_time, \"%H:%M\")\n    opening_time = datetime.strptime(opening_time, \"%H:%M\")\n    closing_time = datetime.strptime(closing_time, \"%H:%M\")\n\n    if opening_time &lt;= user_time &lt;= closing_time:\n        time_difference = closing_time - user_time\n        hours_difference = time_difference.total_seconds() / 3600  # Convert to hours\n        if hours_difference &gt;= 4:\n            return 1\n        elif hours_difference &gt; 0:\n            return 0.5\n        else:\n            return 0\n    else:\n        return 0\n</code></pre>"},{"location":"sim/#intellikit.sim.vector_cosine_similarity","title":"<code>vector_cosine_similarity(v1, v2)</code>","text":"<p>Compute cosine similarity between two vectors. (For sentences use sent_cosine_similarity)</p> <p>Parameters:</p> Name Type Description Default <code>v1</code> <code>array-like</code> <p>The first vector.</p> required <code>v2</code> <code>array-like</code> <p>The second vector.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The cosine similarity between the two vectors.</p> <p>Notes</p> <p>Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It is defined as the cosine of the angle between the two vectors.</p> <p>The cosine similarity ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (i.e., have no similarity), and -1 indicates that the vectors are diametrically opposed (i.e., have maximum dissimilarity).</p> <p>This function assumes that the input vectors are non-zero and have the same length.</p> Source code in <code>intellikit/sim.py</code> <pre><code>def vector_cosine_similarity(v1, v2):\n    \"\"\"\n    Compute cosine similarity between two vectors. (For sentences use sent_cosine_similarity)\n\n    Parameters:\n        v1 (array-like): The first vector.\n        v2 (array-like): The second vector.\n\n    Returns:\n        float: The cosine similarity between the two vectors.\n\n    Notes:\n        Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space.\n        It is defined as the cosine of the angle between the two vectors.\n\n        The cosine similarity ranges from -1 to 1, where 1 indicates that the vectors are identical,\n        0 indicates that the vectors are orthogonal (i.e., have no similarity), and -1 indicates that the vectors\n        are diametrically opposed (i.e., have maximum dissimilarity).\n\n        This function assumes that the input vectors are non-zero and have the same length.\n    \"\"\"\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    return dot_product / (norm_v1 * norm_v2)\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use intellikit in a project:</p> <pre><code>import intellikit\n</code></pre>"},{"location":"examples/Document_Search_System/","title":"Document Search System","text":"In\u00a0[1]: Copied! <pre>#Loading libraries. Ensure that you already have Levenshtein and sentence_transformers already installed\n# !pip install Levenshtein\n# !pip install sentence_transformers\nimport intellikit as ik\nimport pandas as pd\nimport numpy as np\n</pre> #Loading libraries. Ensure that you already have Levenshtein and sentence_transformers already installed # !pip install Levenshtein # !pip install sentence_transformers import intellikit as ik import pandas as pd import numpy as np  <pre>/home/runner/.local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm, trange\n</pre> In\u00a0[2]: Copied! <pre># Example documents and query\ndocuments = [\n    \"This is an example document about natural language processing.\",\n    \"Python is a popular programming language for machine learning tasks.\",\n    \"Machine learning algorithms can be implemented in various programming languages.\",\n    \"Natural language processing helps computers understand human language.\",\n    \"Deep learning is a subset of machine learning algorithms.\",\n]\n\nquery = \"programming languages for machine learning\"\n</pre> # Example documents and query documents = [     \"This is an example document about natural language processing.\",     \"Python is a popular programming language for machine learning tasks.\",     \"Machine learning algorithms can be implemented in various programming languages.\",     \"Natural language processing helps computers understand human language.\",     \"Deep learning is a subset of machine learning algorithms.\", ]  query = \"programming languages for machine learning\"   In\u00a0[3]: Copied! <pre>#Using the Vector Space Model (TF-IDF)\ntop_similar_docs_vsm = ik.vector_space_model(query, documents, k=3)\n\n#To print the retrieved documents\nprint(\"Top 3 similar documents:\")\nfor doc, similarity in top_similar_docs_vsm:\n    print(f\"Similarity: {similarity:.4f} - Document: {doc}\")\n</pre> #Using the Vector Space Model (TF-IDF) top_similar_docs_vsm = ik.vector_space_model(query, documents, k=3)  #To print the retrieved documents print(\"Top 3 similar documents:\") for doc, similarity in top_similar_docs_vsm:     print(f\"Similarity: {similarity:.4f} - Document: {doc}\") <pre>Top 3 similar documents:\nSimilarity: 0.3770 - Document: Python is a popular programming language for machine learning tasks.\nSimilarity: 0.2767 - Document: Deep learning is a subset of machine learning algorithms.\nSimilarity: 0.2705 - Document: Machine learning algorithms can be implemented in various programming languages.\n</pre> In\u00a0[4]: Copied! <pre># Using the BM25 Model\ntop_similar_docs_bm25 = ik.bm25(query, documents, k=3)\n\n#To print the retrieved documents\nprint(\"Top 3 similar documents using BM25:\")\nfor doc, score in top_similar_docs_bm25:\n    print(f\"Score: {score:.4f} - Document: {doc}\")\n</pre> # Using the BM25 Model top_similar_docs_bm25 = ik.bm25(query, documents, k=3)  #To print the retrieved documents print(\"Top 3 similar documents using BM25:\") for doc, score in top_similar_docs_bm25:     print(f\"Score: {score:.4f} - Document: {doc}\") <pre>Top 3 similar documents using BM25:\nScore: 21.7943 - Document: This is an example document about natural language processing.\nScore: 20.3793 - Document: Natural language processing helps computers understand human language.\nScore: 19.3249 - Document: Machine learning algorithms can be implemented in various programming languages.\n</pre> In\u00a0[5]: Copied! <pre># Using Sentence Transformers\n# The default model used here is the pre-trained \"paraphrase-MiniLM-L6-v2\" from huggingface hb. Other pre-trained models can be applied.\ntop_similar_docs_st = ik.sentence_transformers_retrieval(query, documents, k=3, model_name='paraphrase-MiniLM-L6-v2')\n\n# To print the retrieved documents\nprint(\"Top 3 similar documents using Sentence Transformers:\")\nfor doc, similarity in top_similar_docs_st:\n    print(f\"Similarity: {similarity:.4f} - Document: {doc}\")\n</pre> # Using Sentence Transformers # The default model used here is the pre-trained \"paraphrase-MiniLM-L6-v2\" from huggingface hb. Other pre-trained models can be applied. top_similar_docs_st = ik.sentence_transformers_retrieval(query, documents, k=3, model_name='paraphrase-MiniLM-L6-v2')  # To print the retrieved documents print(\"Top 3 similar documents using Sentence Transformers:\") for doc, similarity in top_similar_docs_st:     print(f\"Similarity: {similarity:.4f} - Document: {doc}\") <pre>Top 3 similar documents using Sentence Transformers:\nSimilarity: 0.7972 - Document: Machine learning algorithms can be implemented in various programming languages.\nSimilarity: 0.6459 - Document: Python is a popular programming language for machine learning tasks.\nSimilarity: 0.5925 - Document: Deep learning is a subset of machine learning algorithms.\n</pre> In\u00a0[6]: Copied! <pre># An example of preparing a dataframme for this task\ndata = {\n    'document_id': [1, 2, 3, 4, 5],\n    'document_title': [\"Document 1\", \"Document 2\", \"Document 3\", \"Document 4\", \"Document 5\"],\n    'document_text': [\n        \"This is the text of Document 1.\",\n        \"Document 2 contains some example text.\",\n        \"The text in Document 3 is different from others.\",\n        \"Document 4 has unique content.\",\n        \"This is a sample text for Document 5.\"\n    ]\n}\n\ndf = pd.DataFrame(data)\n\n# Function to prepare data for retrieval models\ndef prepare_data(df):\n    documents = df['document_text'].tolist()\n    titles = df['document_title'].tolist()\n    return documents, titles\n\n# Example usage\ndocuments, titles = prepare_data(df)\n\n# Test the BM25 function\nquery = \"example text\"\ntop_similar_docs_bm25 = ik.bm25(query, documents, k=3)\nprint(\"Top 3 similar documents using BM25:\")\nfor doc, score in top_similar_docs_bm25:\n    doc_title = titles[documents.index(doc)]\n    print(f\"Score: {score:.4f} - Document Title: {doc_title}\")\n\n# Test the Vector Space Model function\ntop_similar_docs_vsm = ik.vector_space_model(query, documents, k=3)\nprint(\"\\nTop 3 similar documents using the Vector Space Model:\")\nfor doc, similarity in top_similar_docs_vsm:\n    doc_title = titles[documents.index(doc)]\n    print(f\"Similarity: {similarity:.4f} - Document Title: {doc_title}\")\n</pre> # An example of preparing a dataframme for this task data = {     'document_id': [1, 2, 3, 4, 5],     'document_title': [\"Document 1\", \"Document 2\", \"Document 3\", \"Document 4\", \"Document 5\"],     'document_text': [         \"This is the text of Document 1.\",         \"Document 2 contains some example text.\",         \"The text in Document 3 is different from others.\",         \"Document 4 has unique content.\",         \"This is a sample text for Document 5.\"     ] }  df = pd.DataFrame(data)  # Function to prepare data for retrieval models def prepare_data(df):     documents = df['document_text'].tolist()     titles = df['document_title'].tolist()     return documents, titles  # Example usage documents, titles = prepare_data(df)  # Test the BM25 function query = \"example text\" top_similar_docs_bm25 = ik.bm25(query, documents, k=3) print(\"Top 3 similar documents using BM25:\") for doc, score in top_similar_docs_bm25:     doc_title = titles[documents.index(doc)]     print(f\"Score: {score:.4f} - Document Title: {doc_title}\")  # Test the Vector Space Model function top_similar_docs_vsm = ik.vector_space_model(query, documents, k=3) print(\"\\nTop 3 similar documents using the Vector Space Model:\") for doc, similarity in top_similar_docs_vsm:     doc_title = titles[documents.index(doc)]     print(f\"Similarity: {similarity:.4f} - Document Title: {doc_title}\") <pre>Top 3 similar documents using BM25:\nScore: 17.9712 - Document Title: Document 3\nScore: 16.8036 - Document Title: Document 5\nScore: 15.4860 - Document Title: Document 1\n\nTop 3 similar documents using the Vector Space Model:\nSimilarity: 0.1745 - Document Title: Document 2\nSimilarity: 0.1601 - Document Title: Document 1\nSimilarity: 0.1488 - Document Title: Document 5\n</pre>"},{"location":"examples/Recommendation_System/","title":"Recommendation System","text":"In\u00a0[1]: Copied! <pre>#!pip install Levenshtein\n#!pip install sentence_transformers\n</pre> #!pip install Levenshtein #!pip install sentence_transformers In\u00a0[2]: Copied! <pre>import intellikit as ik\nimport pandas as pd\n</pre> import intellikit as ik import pandas as pd <pre>/home/runner/.local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm, trange\n</pre> In\u00a0[3]: Copied! <pre># Define your DataFrame on which the recommendation system is going to be based\ndf = pd.DataFrame({\n    'feature1': ['apple', 'orange', 'banana', 'grape'],\n    'feature2': ['red', 'green', 'blue', 'yellow'],\n    'feature3': ['small', 'large', 'medium', 'small'],\n    'feature4': [1, 2, 3, 4],\n    'feature5': [10, 20, 30, 40]\n})\n\n#Define your query \nquery = pd.DataFrame({\n    'feature1': ['apple'],\n    'feature2': ['yellow'],\n    'feature3': ['small'],\n    'feature4': [3],\n    'feature5': [30]\n\n})\n\n#Define you similarity calculation methods for your project\nhamming = ik.sim_hamming\nlevenshtein = ik.sim_levenshtein\nlevel = ik.sim_level\nabs_diff = ik.sim_difference\n\n# Assign the appro*priate similarity calculation functions to each feature\nsimilarity_functions = {\n    'feature1': hamming,\n    'feature2': levenshtein,\n    'feature3': level,\n    'feature4': abs_diff,\n    'feature5': abs_diff\n}\n</pre> # Define your DataFrame on which the recommendation system is going to be based df = pd.DataFrame({     'feature1': ['apple', 'orange', 'banana', 'grape'],     'feature2': ['red', 'green', 'blue', 'yellow'],     'feature3': ['small', 'large', 'medium', 'small'],     'feature4': [1, 2, 3, 4],     'feature5': [10, 20, 30, 40] })  #Define your query  query = pd.DataFrame({     'feature1': ['apple'],     'feature2': ['yellow'],     'feature3': ['small'],     'feature4': [3],     'feature5': [30]  })  #Define you similarity calculation methods for your project hamming = ik.sim_hamming levenshtein = ik.sim_levenshtein level = ik.sim_level abs_diff = ik.sim_difference  # Assign the appro*priate similarity calculation functions to each feature similarity_functions = {     'feature1': hamming,     'feature2': levenshtein,     'feature3': level,     'feature4': abs_diff,     'feature5': abs_diff } In\u00a0[4]: Copied! <pre># Applying the methods and weights and retrieving the top results using the linear retriever\nfeature_weights = {\n    'feature1': 0.4,\n    'feature2': 0.2,\n    'feature3': 0.1,\n    'feature4': 0.1,\n    'feature5': 0.2\n}\n\ntop_n = 2  # Number of top similar results to return\ntop_similar_cases_linear = ik.linearRetriever(df, query, similarity_functions, feature_weights, top_n)\nprint(\"Top similar cases (Linear):\")\nprint(top_similar_cases_linear)\n</pre> # Applying the methods and weights and retrieving the top results using the linear retriever feature_weights = {     'feature1': 0.4,     'feature2': 0.2,     'feature3': 0.1,     'feature4': 0.1,     'feature5': 0.2 }  top_n = 2  # Number of top similar results to return top_similar_cases_linear = ik.linearRetriever(df, query, similarity_functions, feature_weights, top_n) print(\"Top similar cases (Linear):\") print(top_similar_cases_linear)  <pre>Top similar cases (Linear):\n  feature1 feature2 feature3  feature4  feature5\n0    apple      red    small         1        10\n3    grape   yellow    small         4        40\n</pre>"},{"location":"examples/intro/","title":"Intro","text":"In\u00a0[1]: Copied! <pre># To install\n# !pip install intellikit\n#To load and use the package\nimport intellikit as ik\n</pre> # To install # !pip install intellikit #To load and use the package import intellikit as ik <pre>/home/runner/.local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm, trange\n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\n\n# Define the questions and answers\nquestions = [\n    \"What are the admission requirements for universities in Uganda?\",\n    \"How can I apply for a scholarship?\",\n    \"What is the curriculum for primary schools in Uganda?\",\n    \"Where can I find information about school fees?\",\n    \"What are the contact details for the Ministry of Education?\"\n]\n\nanswers = [\n    \"The admission requirements for universities in Uganda vary depending on the institution. It is recommended to visit the website of the specific university you are interested in for detailed information.\",\n    \"Scholarship applications are usually managed by different organizations and institutions. You can check the Ministry of Education's website for information on available scholarships and application procedures.\",\n    \"The curriculum for primary schools in Uganda is designed by the National Curriculum Development Centre (NCDC). You can visit their website for detailed information on the curriculum.\",\n    \"Information about school fees can be obtained from the respective schools or educational institutions. It is recommended to contact the schools directly for accurate and up-to-date fee information.\",\n    \"You can find the contact details for the Ministry of Education on their official website. They usually provide phone numbers, email addresses, and physical addresses for different departments and offices.\"\n]\n\n# Create the dataframe\ndf = pd.DataFrame({'Question': questions, 'Answer': answers})\n\n# Print the dataframe\nprint(df)\n</pre> import pandas as pd  # Define the questions and answers questions = [     \"What are the admission requirements for universities in Uganda?\",     \"How can I apply for a scholarship?\",     \"What is the curriculum for primary schools in Uganda?\",     \"Where can I find information about school fees?\",     \"What are the contact details for the Ministry of Education?\" ]  answers = [     \"The admission requirements for universities in Uganda vary depending on the institution. It is recommended to visit the website of the specific university you are interested in for detailed information.\",     \"Scholarship applications are usually managed by different organizations and institutions. You can check the Ministry of Education's website for information on available scholarships and application procedures.\",     \"The curriculum for primary schools in Uganda is designed by the National Curriculum Development Centre (NCDC). You can visit their website for detailed information on the curriculum.\",     \"Information about school fees can be obtained from the respective schools or educational institutions. It is recommended to contact the schools directly for accurate and up-to-date fee information.\",     \"You can find the contact details for the Ministry of Education on their official website. They usually provide phone numbers, email addresses, and physical addresses for different departments and offices.\" ]  # Create the dataframe df = pd.DataFrame({'Question': questions, 'Answer': answers})  # Print the dataframe print(df)  <pre>                                            Question  \\\n0  What are the admission requirements for univer...   \n1                 How can I apply for a scholarship?   \n2  What is the curriculum for primary schools in ...   \n3    Where can I find information about school fees?   \n4  What are the contact details for the Ministry ...   \n\n                                              Answer  \n0  The admission requirements for universities in...  \n1  Scholarship applications are usually managed b...  \n2  The curriculum for primary schools in Uganda i...  \n3  Information about school fees can be obtained ...  \n4  You can find the contact details for the Minis...  \n</pre> In\u00a0[3]: Copied! <pre>#Define you similarity calculation methods for your project\ncosine = ik.sim_sentence_cosine\n\n# Assign the similarity calculation function to the feature\nsimilarity_functions = {\n    'Question': cosine\n}\n\n# Weighting out feature. Since this is just one feature the default weight of one should be used. \nfeature_weights = {\n    'Question': 1\n}\n\n#How many results do we need. Just one is enough since this is a QA system. You could also opt for multiple results to give users multiple options\ntop_n = 1\n</pre> #Define you similarity calculation methods for your project cosine = ik.sim_sentence_cosine  # Assign the similarity calculation function to the feature similarity_functions = {     'Question': cosine }  # Weighting out feature. Since this is just one feature the default weight of one should be used.  feature_weights = {     'Question': 1 }  #How many results do we need. Just one is enough since this is a QA system. You could also opt for multiple results to give users multiple options top_n = 1 In\u00a0[4]: Copied! <pre>#Define your query \nquery = pd.DataFrame({\n    'Question': ['Where can i apply for a scholarship?']\n\n})\n\nreturned_df = ik.linearRetriever(df, query, similarity_functions, feature_weights, top_n)\nreturned_dict = ik.dataframe_to_dict(df=returned_df, orientation=\"records\")\nreturned_dict\n</pre> #Define your query  query = pd.DataFrame({     'Question': ['Where can i apply for a scholarship?']  })  returned_df = ik.linearRetriever(df, query, similarity_functions, feature_weights, top_n) returned_dict = ik.dataframe_to_dict(df=returned_df, orientation=\"records\") returned_dict Out[4]: <pre>[{'Question': 'How can I apply for a scholarship?',\n  'Answer': \"Scholarship applications are usually managed by different organizations and institutions. You can check the Ministry of Education's website for information on available scholarships and application procedures.\"}]</pre> In\u00a0[5]: Copied! <pre>response = returned_dict[0]['Answer']\nik.stream_text(response)\n</pre> response = returned_dict[0]['Answer'] ik.stream_text(response)  <pre>S</pre> <pre>c</pre> <pre>h</pre> <pre>o</pre> <pre>l</pre> <pre>a</pre> <pre>r</pre> <pre>s</pre> <pre>h</pre> <pre>i</pre> <pre>p</pre> <pre> </pre> <pre>a</pre> <pre>p</pre> <pre>p</pre> <pre>l</pre> <pre>i</pre> <pre>c</pre> <pre>a</pre> <pre>t</pre> <pre>i</pre> <pre>o</pre> <pre>n</pre> <pre>s</pre> <pre> </pre> <pre>a</pre> <pre>r</pre> <pre>e</pre> <pre> </pre> <pre>u</pre> <pre>s</pre> <pre>u</pre> <pre>a</pre> <pre>l</pre> <pre>l</pre> <pre>y</pre> <pre> </pre> <pre>m</pre> <pre>a</pre> <pre>n</pre> <pre>a</pre> <pre>g</pre> <pre>e</pre> <pre>d</pre> <pre> </pre> <pre>b</pre> <pre>y</pre> <pre> </pre> <pre>d</pre> <pre>i</pre> <pre>f</pre> <pre>f</pre> <pre>e</pre> <pre>r</pre> <pre>e</pre> <pre>n</pre> <pre>t</pre> <pre> </pre> <pre>o</pre> <pre>r</pre> <pre>g</pre> <pre>a</pre> <pre>n</pre> <pre>i</pre> <pre>z</pre> <pre>a</pre> <pre>t</pre> <pre>i</pre> <pre>o</pre> <pre>n</pre> <pre>s</pre> <pre> </pre> <pre>a</pre> <pre>n</pre> <pre>d</pre> <pre> </pre> <pre>i</pre> <pre>n</pre> <pre>s</pre> <pre>t</pre> <pre>i</pre> <pre>t</pre> <pre>u</pre> <pre>t</pre> <pre>i</pre> <pre>o</pre> <pre>n</pre> <pre>s</pre> <pre>.</pre> <pre> </pre> <pre>Y</pre> <pre>o</pre> <pre>u</pre> <pre> </pre> <pre>c</pre> <pre>a</pre> <pre>n</pre> <pre> </pre> <pre>c</pre> <pre>h</pre> <pre>e</pre> <pre>c</pre> <pre>k</pre> <pre> </pre> <pre>t</pre> <pre>h</pre> <pre>e</pre> <pre> </pre> <pre>M</pre> <pre>i</pre> <pre>n</pre> <pre>i</pre> <pre>s</pre> <pre>t</pre> <pre>r</pre> <pre>y</pre> <pre> </pre> <pre>o</pre> <pre>f</pre> <pre> </pre> <pre>E</pre> <pre>d</pre> <pre>u</pre> <pre>c</pre> <pre>a</pre> <pre>t</pre> <pre>i</pre> <pre>o</pre> <pre>n</pre> <pre>'</pre> <pre>s</pre> <pre> </pre> <pre>w</pre> <pre>e</pre> <pre>b</pre> <pre>s</pre> <pre>i</pre> <pre>t</pre> <pre>e</pre> <pre> </pre> <pre>f</pre> <pre>o</pre> <pre>r</pre> <pre> </pre> <pre>i</pre> <pre>n</pre> <pre>f</pre> <pre>o</pre> <pre>r</pre> <pre>m</pre> <pre>a</pre> <pre>t</pre> <pre>i</pre> <pre>o</pre> <pre>n</pre> <pre> </pre> <pre>o</pre> <pre>n</pre> <pre> </pre> <pre>a</pre> <pre>v</pre> <pre>a</pre> <pre>i</pre> <pre>l</pre> <pre>a</pre> <pre>b</pre> <pre>l</pre> <pre>e</pre> <pre> </pre> <pre>s</pre> <pre>c</pre> <pre>h</pre> <pre>o</pre> <pre>l</pre> <pre>a</pre> <pre>r</pre> <pre>s</pre> <pre>h</pre> <pre>i</pre> <pre>p</pre> <pre>s</pre> <pre> </pre> <pre>a</pre> <pre>n</pre> <pre>d</pre> <pre> </pre> <pre>a</pre> <pre>p</pre> <pre>p</pre> <pre>l</pre> <pre>i</pre> <pre>c</pre> <pre>a</pre> <pre>t</pre> <pre>i</pre> <pre>o</pre> <pre>n</pre> <pre> </pre> <pre>p</pre> <pre>r</pre> <pre>o</pre> <pre>c</pre> <pre>e</pre> <pre>d</pre> <pre>u</pre> <pre>r</pre> <pre>e</pre> <pre>s</pre> <pre>.</pre> In\u00a0[6]: Copied! <pre>query = pd.DataFrame({\n    'Question': ['How do i contact the ministry of education?']\n\n})\n\nreturned_df = ik.linearRetriever(df, query, similarity_functions, feature_weights, top_n)\nreturned_dict = ik.dataframe_to_dict(df=returned_df, orientation=\"records\")\nresponse = returned_dict[0]['Answer']\nik.stream_text(response) #returns the response in an animated manner like chatgpt\n</pre> query = pd.DataFrame({     'Question': ['How do i contact the ministry of education?']  })  returned_df = ik.linearRetriever(df, query, similarity_functions, feature_weights, top_n) returned_dict = ik.dataframe_to_dict(df=returned_df, orientation=\"records\") response = returned_dict[0]['Answer'] ik.stream_text(response) #returns the response in an animated manner like chatgpt <pre>Y</pre> <pre>o</pre> <pre>u</pre> <pre> </pre> <pre>c</pre> <pre>a</pre> <pre>n</pre> <pre> </pre> <pre>f</pre> <pre>i</pre> <pre>n</pre> <pre>d</pre> <pre> </pre> <pre>t</pre> <pre>h</pre> <pre>e</pre> <pre> </pre> <pre>c</pre> <pre>o</pre> <pre>n</pre> <pre>t</pre> <pre>a</pre> <pre>c</pre> <pre>t</pre> <pre> </pre> <pre>d</pre> <pre>e</pre> <pre>t</pre> <pre>a</pre> <pre>i</pre> <pre>l</pre> <pre>s</pre> <pre> </pre> <pre>f</pre> <pre>o</pre> <pre>r</pre> <pre> </pre> <pre>t</pre> <pre>h</pre> <pre>e</pre> <pre> </pre> <pre>M</pre> <pre>i</pre> <pre>n</pre> <pre>i</pre> <pre>s</pre> <pre>t</pre> <pre>r</pre> <pre>y</pre> <pre> </pre> <pre>o</pre> <pre>f</pre> <pre> </pre> <pre>E</pre> <pre>d</pre> <pre>u</pre> <pre>c</pre> <pre>a</pre> <pre>t</pre> <pre>i</pre> <pre>o</pre> <pre>n</pre> <pre> </pre> <pre>o</pre> <pre>n</pre> <pre> </pre> <pre>t</pre> <pre>h</pre> <pre>e</pre> <pre>i</pre> <pre>r</pre> <pre> </pre> <pre>o</pre> <pre>f</pre> <pre>f</pre> <pre>i</pre> <pre>c</pre> <pre>i</pre> <pre>a</pre> <pre>l</pre> <pre> </pre> <pre>w</pre> <pre>e</pre> <pre>b</pre> <pre>s</pre> <pre>i</pre> <pre>t</pre> <pre>e</pre> <pre>.</pre> <pre> </pre> <pre>T</pre> <pre>h</pre> <pre>e</pre> <pre>y</pre> <pre> </pre> <pre>u</pre> <pre>s</pre> <pre>u</pre> <pre>a</pre> <pre>l</pre> <pre>l</pre> <pre>y</pre> <pre> </pre> <pre>p</pre> <pre>r</pre> <pre>o</pre> <pre>v</pre> <pre>i</pre> <pre>d</pre> <pre>e</pre> <pre> </pre> <pre>p</pre> <pre>h</pre> <pre>o</pre> <pre>n</pre> <pre>e</pre> <pre> </pre> <pre>n</pre> <pre>u</pre> <pre>m</pre> <pre>b</pre> <pre>e</pre> <pre>r</pre> <pre>s</pre> <pre>,</pre> <pre> </pre> <pre>e</pre> <pre>m</pre> <pre>a</pre> <pre>i</pre> <pre>l</pre> <pre> </pre> <pre>a</pre> <pre>d</pre> <pre>d</pre> <pre>r</pre> <pre>e</pre> <pre>s</pre> <pre>s</pre> <pre>e</pre> <pre>s</pre> <pre>,</pre> <pre> </pre> <pre>a</pre> <pre>n</pre> <pre>d</pre> <pre> </pre> <pre>p</pre> <pre>h</pre> <pre>y</pre> <pre>s</pre> <pre>i</pre> <pre>c</pre> <pre>a</pre> <pre>l</pre> <pre> </pre> <pre>a</pre> <pre>d</pre> <pre>d</pre> <pre>r</pre> <pre>e</pre> <pre>s</pre> <pre>s</pre> <pre>e</pre> <pre>s</pre> <pre> </pre> <pre>f</pre> <pre>o</pre> <pre>r</pre> <pre> </pre> <pre>d</pre> <pre>i</pre> <pre>f</pre> <pre>f</pre> <pre>e</pre> <pre>r</pre> <pre>e</pre> <pre>n</pre> <pre>t</pre> <pre> </pre> <pre>d</pre> <pre>e</pre> <pre>p</pre> <pre>a</pre> <pre>r</pre> <pre>t</pre> <pre>m</pre> <pre>e</pre> <pre>n</pre> <pre>t</pre> <pre>s</pre> <pre> </pre> <pre>a</pre> <pre>n</pre> <pre>d</pre> <pre> </pre> <pre>o</pre> <pre>f</pre> <pre>f</pre> <pre>i</pre> <pre>c</pre> <pre>e</pre> <pre>s</pre> <pre>.</pre>"}]}